# 《自然语言处理》实验2报告

## 1. 实验目标

- 对中文文本进行分词处理
- 使用TF-IDF算法进行关键词提取和词频统计
- 基于TF-IDF权重生成彩色词云，直观展示各文档关键词

## 2. 实验环境和工具

- **编程语言**: Python 3.x
- **主要库**:
  - `jieba`: 中文分词
  - `wordcloud`: 生成词云
  - `matplotlib`: 图形可视化
  - `numpy`: 数学计算
  - `re`: 正则表达式处理

## 3. 实验原理

### 3.1 TF-IDF算法简介

TF-IDF(Term Frequency-Inverse Document Frequency)是一种用于评估词语对于文档集合中某篇文档重要性的统计方法。其计算基于以下公式：

- 词频(TF): TF(t,d) = 词t在文档d中出现的次数
- 逆文档频率(IDF): IDF(t,D) = log(总文档数/(包含词t的文档数+1))
- TF-IDF: TF-IDF(t,d,D) = TF(t,d) × IDF(t,D)

### 3.2 词云生成原理

词云是一种文本数据可视化方式，通过调整词语的大小、颜色等属性来直观展示其在文本中的重要性。本实验中，词语的大小由其TF-IDF值决定，颜色则基于权重分级。

## 4. 实验步骤与实现

### 4.1 文本预处理

文本预处理主要包括以下步骤：

```python
def preprocess_text(text):
    # 去除文件路径行
    text = re.sub(r'// filepath:.*', '', text)
    # 去除数字、标点符号和特殊字符，只保留中文
    text = re.sub(r'[^\u4e00-\u9fa5]+', ' ', text)
    return text
```

### 4.2 分词处理

使用jieba分词器进行中文分词，并过滤停用词：

```python
def segment_text(text, stopwords):
    # 使用jieba的精确模式分词
    words = jieba.lcut(text)
    
    # 过滤停用词和短词
    filtered_words = [word for word in words if word.strip() and word not in stopwords and len(word) > 1]
    return filtered_words
```

### 4.3 自定义TF-IDF实现

本实验使用手动实现的TF-IDF计算方法，而非依赖于scikit-learn库，确保计算过程的透明性和可控性：

```python
def calculate_tf_idf_for_file(file_path, stopwords):
    # 读取并预处理文本
    text = read_file(file_path)
    preprocessed_text = preprocess_text(text)
    words = segment_text(preprocessed_text, stopwords)
    
    # 计算词频 (TF)
    word_counts = Counter(words)
    tf = {word: count for word, count in word_counts.items()}
    
    return tf, os.path.basename(file_path)

def calculate_idf(all_document_words, corpus_files):
    document_frequency = {}  # 记录每个词出现在多少个文档中
    vocabulary = set()
    
    # 统计词汇表和文档频率
    for doc_words in all_document_words:
        unique_words = set(doc_words.keys())
        vocabulary.update(unique_words)
        
        for word in unique_words:
            if word in document_frequency:
                document_frequency[word] += 1
            else:
                document_frequency[word] = 1
    
    # 计算IDF: log(总文档数/(包含词t的文档数+1))
    total_docs = len(corpus_files)
    idf = {}
    for word in vocabulary:
        idf[word] = math.log(total_docs / (document_frequency[word] + 1))
        
    return idf, list(vocabulary)
```

### 4.4 多彩词云生成

本实验基于TF-IDF权重生成彩色词云，使用权重分级的颜色映射方案：

```python
def generate_wordcloud(word_weights, output_path, font_path, title=None, top_n=20):
    # 只保留前top_n个词        
    top_words = dict(sorted(word_weights.items(), key=lambda x: x[1], reverse=True)[:top_n])
    
    # 创建自定义颜色函数，根据词语权重动态分配颜色
    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):
        weight = top_words.get(word, 0)
        max_weight = max(top_words.values()) if top_words else 1
        normalized_weight = weight / max_weight
        
        # 使用颜色梯度：红色(高权重) -> 紫色(中等权重) -> 蓝色(低权重)
        if normalized_weight > 0.7:
            return "rgb(220, 20, 60)"  # 猩红色 - 高权重
        elif normalized_weight > 0.4:
            return "rgb(148, 0, 211)"  # 紫色 - 中等权重
        else:
            return "rgb(30, 144, 255)"  # 道奇蓝 - 低权重
```

## 5. 实验结果

### 5.2 词云可视化效果

本实验生成了两类词云：
1. **单文档词云**：反映单个文档的关键词分布，突出显示该文档的主题和特点。
2. **综合词云**：将所有文档的TF-IDF值合并后生成，反映整个文档集合的共同主题和关键概念。

词云中，词语大小反映其TF-IDF权重，颜色则基于权重分级：红色表示高权重词语，紫色表示中等权重词语，蓝色表示低权重词语。

![单文档词云示例](/root/NLP/lab2/output/wordclouds/doc1_wordcloud.png)
![综合词云示例](/root/NLP/lab2/output/wordcloud_overall.png)

## 6. 实验创新点

1. **自定义TF-IDF实现**：不依赖于第三方库，手动实现TF-IDF算法，确保计算过程的透明性和可控性。

2. **多级权重颜色映射**：根据TF-IDF权重使用三级颜色映射方案，增强词云的视觉效果和信息传达能力。

3. **多文档综合分析**：不仅分析单个文档的关键词，还合并所有文档的TF-IDF值生成综合词云，提供整体视角。

4. **权重优化处理**：对极小权重值进行缩放处理，确保词云生成的稳定性和可视化效果。